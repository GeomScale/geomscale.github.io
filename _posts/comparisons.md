---
layout: single
title: "Comparing volesti with various software"
date: 2021-05-18
author: Apostolos Chalkis and Vissarion Fisikopoulos
author_profile: true
read_time: true
comments: true
share: true
related: true
hidden: false
---


# Comparing volesti with various software

In this blog post we will compare volesti sampling routines with other R packages the provide sampling
methods for multivariate distributions, volume computation for convex polytopes and integral calculation
for multivariate functions defined over o convex polytope. 

[*volesti*](https://github.com/GeomScale/volume_approximation)  is a C++ library for volume approximation and sampling of convex bodies (*e.g.* polytopes) with an R  interface released in CRAN. *volesti* is part of the [GeomScale](https://geomscale.github.io/) project.  

The goal of this blog post is to show that *volesti* is the most efficient choice, among R packages, to sample from the uniform or a restricted Gaussian distribution over a polytope or to estimate the volume of a polytope fast.

In particular, we will compare volesti with:

1. *hitandrun*. This package uses Hit-and-Run algorithm to sample uniformly from a convex polytope.
2. *restrictedMVN*. This package uses Gibbs sampler for a multivariate normal restricted in a convex polytope.
3. *tmg*. This package uses exact Hamiltonian Monte Carlo with boundary reflections to sample from a a multivariate normal restricted in a convex polytope.
4. *geometry*. This package is an R interface of C++ library *qhull*. It uses a deterministic algorithm to compute the volume of a convex polytope.
5. *SimplicialCubature*. This package  integrate functions over m-dimensional simplices in n-dimensional Euclidean space.

## Against hitandrun

The following script samples 1000 points from the 100-dimensional hypercube $$[-1,1]^100$$. Then, we count the performance of each package. By this we mean the number of *effective sample size* (ESS) per second. The ESS of an autocorrelated  sample, generated by a Markov Chain Monte Carlo sampling algorithm, is the number of independent samples that it is equivalent to. For more information about ESS you can read this [paper](https://si.biostat.washington.edu/sites/default/files/modules/Geyer-Introduction%20to%20markov%20chain%20Monte%20Carlo_0.pdf). To estimate the effective sample size we use the package [*coda*]([https://CRAN.R-project.org/package=coda](https://cran.r-project.org/package=coda)). 

```R
d = 100
P = gen_cube(d, 'H')

constr = list("constr" = P@A, "dir" = rep("<=", 2 * d), "rhs" = P@b)
time1 = system.time({ samples1 = t(hitandrun::hitandrun(constr = constr, 
    	n.samples = 1000, thin = 1)) })
time2 = system.time({ samples2 = sample_points(P, random_walk = list(
    	"walk" = "RDHR", "walk_length" = 1, "seed" = 5), n = 1000) })                
cat(min(coda::effectiveSize(t(samples1))) / time1[3],
    min(coda::effectiveSize(t(samples2))) / time2[3])
```

And the output of this script is,

```
0.02818878 74.18608
```

This means that the performance of *volesti* is ~2500 times faster than the performance of *hitandrun*.



## Against restrictedMVN and tmg

In many Bayesian models the posterior distribution is a multivariate Gaussian distribution restricted to a specific domain. We illustrate the efficiency of *volesti* for the case of the truncation being the canonical simplex,

<center>
$$\Delta^n =\{ x\in\R^n\ |\ x_i\geq 0,\ \sum_ix_i=1 \}$$
<center>



This case is of special interest. This situation typically occurs whenever the unknown parameters can be interpreted as fractions or probabilities. Thus, it appears in many important applications; for more details you can read this [paper](https://ieeexplore.ieee.org/document/6884588).

The probability density function we are going to sample from in the following example is,

```html
<center>
$$f(x|\mu,\Sigma) \propto  \left\{
\begin{array}{ll}
      exp[-\frac{1}{2}(x-\mu)^T\Sigma(x-\mu)],  & \mbox{ if } x\in\Delta^n ,\\
      0, & \mbox{otherwise.}\\
\end{array} 
\right.  $$
<center>
```

The following script applies the necessary linear transformations and then uses the three packages to sample from the previous probability density function.

```R
d = 100
S = matrix( rnorm(d*d,mean=0,sd=1), d, d) #random covariance matrix 
S = S %*% t(S)
shift = rep(1/d, d)
A = -diag(d)
b = rep(0,d)
b = b - A %*% shift
Aeq = t(as.matrix(rep(1,d), 10,1))
N = pracma::nullspace(Aeq)       
A = A %*% N #transform the truncation into a full dimensional polytope
S = t(N) %*% S %*% N
A = A %*% t(chol(S)) #Cholesky decomposition to transform to the standard Gaussian
P = Hpolytope(A=A, b=as.numeric(b)) #new truncation

time1 = system.time({samples1 = sample_points(P, n = 100000, random_walk = 
    list("walk"="CDHR", "burn-in"=1000, "starting_point" = rep(0, d-1), "seed" = 
    127), distribution = list("density" = "gaussian", "mode" = rep(0, d-1))) })
# tmg does not terminate
time2 = system.time({samples2 = tmg::rtmg(n = 100000, M = diag(d-1), r = 
    rep(0, d-1), initial = rep(0, d-1), P@A, P@b, q = NULL, burn.in = 1000) })            
time3 = system.time({samples3 = restrictedMVN::sample_from_constraints(
    linear_part = A, offset= b, mean_param = rep(0,d-1), covariance = diag(d-1), 
    initial_point = inner_ball(P)[1:(d-1)], ndraw=100000, burnin=1000) })   
                  
# print the performance (effective sample size over time)
cat(min(coda::effectiveSize(t(samples1))) / time1[3],
    min(coda::effectiveSize(samples3)) / time3[3])
```

